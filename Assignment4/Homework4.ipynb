{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#Question 1 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The problem explicitly states NOT to find the pdf of the posterior, but I was unable to come up with a cleverer way to do this on my own. Therefore, here is a derivation that I adapted from these sources:\n",
    "http://krasserm.github.io/2018/03/19/gaussian-processes/,https://see.stanford.edu/materials/aimlcs229/cs229-gp.pdf,https://www.csie.ntu.edu.tw/~cjlin/mlgroup/tutorials/gpr.pdf,http://web.stanford.edu/class/stats200/Lecture20.pdf\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "\n",
    "We can show the mean as the following:\n",
    "\n",
    "$E(y^{*}|X^{*},X,y) = E(y^{*} + -K(X^{*},X)K(X,X)^{-1}y--K(X^{*},X)K(X,X)^{-1}y|X^{*},X,y) = E(y^{*} + -K(X^{*},X)K(X,X)^{-1}y|X^{*},X,y) - E(-K(X^{*},X)K(X,X)^{-1}|X^{*},X,y) = --K(X^{*},X)K(X,X)^{-1}y = -K(X^{*},X)K(X,X)^{-1}y$\n",
    "\n",
    "And the variance as:\n",
    "\n",
    "$Var(y^{*}|X^{*},X,y)$\n",
    "\n",
    "$=Var(y^{*} + -K(X^{*},X)K(X,X)^{-1}y|X^{*},X,y) + Var(-K(X^{*},X)K(X,X)^{-1}y|X^{*},X,y) - Cov(y^{*} + -K(X^{*},X)K(X,X)^{-1}y,--K(X^{*},X)K(X,X)^{-1}y|X^{*},X,y) - Cov(--K(X^{*},X)K(X,X)^{-1}y,y^{*} + -K(X^{*},X)K(X,X)^{-1}y|X^{*},X,y)$\n",
    "\n",
    "$=Var(y^{*}+-K(X^{*},X)K(X,X)^{-1}y|X^{*},X)$\n",
    "\n",
    "$=Var(y^{*}|X^{*},X) + Var(-K(X^{*},X)K(X,X)^{-1}y,X^{*},X) - Cov(y^{*},-K(X^{*},X)K(X,X)^{-1}y|X^{*},X) - Cov(-K(X^{*},X)K(X,X)^{-1}y,y^{*}|X^{*},X)$\n",
    "\n",
    "$=K(X^{*},X^{*}) - K(X^{*},X)K(X,X)^{-1}K(X,X^{*})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "source": [
    "For this question, I used the following resources:\n",
    "http://krasserm.github.io/2018/03/19/gaussian-processes/, http://www.gaussianprocess.org/gpml/chapters/RW5.pdf, and\n",
    "https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf\n",
    "\n",
    "**no more gamma\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  26.27670484   36.09550331   36.09550331 ...   36.09550331\n    36.09550331   36.09550331]\n [  36.09550331    3.46251263   36.09550331 ...   36.09550331\n    36.09550331   36.09550331]\n [  36.09550331   36.09550331   29.82764022 ...   36.09550331\n    36.09550331   36.09550331]\n ...\n [  36.09550331   36.09550331   36.09550331 ...   23.10989836\n    36.09550331   36.09550331]\n [  36.09550331   36.09550331   36.09550331 ...   36.09550331\n    41.57009984   36.09550331]\n [  36.09550331   36.09550331   36.09550331 ...   36.09550331\n    36.09550331 -204.64121683]] mean\n[[9.94113453e-001 9.01707893e-312 0.00000000e+000 ... 3.13691184e-109\n  5.68545337e-220 6.42874404e-198]\n [9.01707893e-312 9.94113453e-001 0.00000000e+000 ... 0.00000000e+000\n  0.00000000e+000 0.00000000e+000]\n [0.00000000e+000 0.00000000e+000 9.94113453e-001 ... 0.00000000e+000\n  0.00000000e+000 0.00000000e+000]\n ...\n [3.13691184e-109 0.00000000e+000 0.00000000e+000 ... 1.33352223e+001\n  1.25271100e-029 6.17455158e-045]\n [5.68545337e-220 0.00000000e+000 0.00000000e+000 ... 1.25271100e-029\n  1.00210673e+000 4.28887605e-019]\n [6.42874404e-198 0.00000000e+000 0.00000000e+000 ... 6.17455158e-045\n  4.28887605e-019 1.68383283e+002]] variance\n36.09550330722694 naive mean\n"
     ]
    }
   ],
   "source": [
    "def RBFKernel(X1,X2,sigma,h): #same RBF kernel function as in the last homework\n",
    "    K = np.zeros((X1.shape[0],X2.shape[0]))\n",
    "    for i in range(X1.shape[0]): \n",
    "        for j in range(X2.shape[0]):\n",
    "            K[i,j] = sigma*np.exp(((-np.linalg.norm(X1[i]-X2[j])**2)/2*(h**2)))\n",
    "    return K\n",
    "\n",
    "def GPRegression(XTrain, yTrain, XTest, sigma, h):\n",
    "    num = np.zeros(XTrain.shape[0])\n",
    "    y_mean = np.mean(yTrain) \n",
    "    y = yTrain - y_mean\n",
    "    K = RBFKernel(XTrain, XTrain, sigma,h) #can repeat this for test data as well\n",
    "    L_ = np.eye(len(num)) #identity matrix\n",
    "    L_n = K + (np.tril(L_,k=0)) \n",
    "    L = np.linalg.cholesky(L_n) #computing the cholesky decomposition\n",
    "    alpha = np.linalg.solve(L.T,np.linalg.solve(L,y)) #solves the matrix eqn for the cholesky decomp and the target\n",
    "    GPMean = y_mean + ((K.T)*alpha) #final regression mean\n",
    "\n",
    "    var = np.linalg.solve(L,K) #solves matrix eqn for the cholesky decomp and the kernel\n",
    "    GPVariance = K - ((var.T)*var) #final regression variance\n",
    "\n",
    "\n",
    "    return GPMean, GPVariance\n",
    "\n",
    "def LogMarginalLikelihood(XTrain,yTrain,sigma,h):\n",
    "    n = np.zeros(XTrain.shape[0],dtype=int)\n",
    "    \n",
    "    ym = np.mean(yTrain)\n",
    "    #print(ym)\n",
    "    y = yTrain - ym\n",
    "    K = RBFKernel(XTrain,XTrain,sigma,h)\n",
    "\n",
    "    #same calculation as before\n",
    "    L_ = np.eye(len(n))\n",
    "    L_n = K + (np.tril(L_,k=0))\n",
    "    L = np.linalg.cholesky(L_n)\n",
    "\n",
    "\n",
    "    alpha = np.linalg.solve(L.T,np.linalg.solve(L,y))\n",
    "    #use equation from second resource\n",
    "    logml = np.mean((-1/2*y.T*alpha)-(sum(np.log(np.diag(L))) - ((n/2)*np.log(2*np.pi))))\n",
    "    return logml,sigma,h\n",
    "\n",
    "def HyperParameters(XTrain,yTrain,hs,sigmas):\n",
    "    #gamma = 0.01*(np.std(yTrain))\n",
    "\n",
    "    margs_list = []\n",
    "    #couldnt figure out how to do this with gridsearch, so I'm doing it manually with a for loop\n",
    "    for i in sigmas:\n",
    "        for j in hs:\n",
    "            lm,s,h = LogMarginalLikelihood(XTrain,yTrain,i,j)\n",
    "            margs_list.append(lm)\n",
    "            if np.min(margs_list) == lm:\n",
    "                new_min = [lm,s,h]\n",
    "            else:\n",
    "                pass\n",
    "    h = new_min[2]\n",
    "    sigma = new_min[1]\n",
    "    return h,sigma\n",
    "\n",
    "\n",
    "data = pd.read_excel('/Users/lvbenson/Research_Projects/MachineLearning/Assignment4/Concrete_Data.xls')\n",
    "X = data.iloc[:,:-1] # Features \n",
    "y = data.iloc[:,-1] # Target \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2) \n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "hs_ = (np.logspace(-1,1,num=10).T)*np.linalg.norm(np.std(X_train)) #from hw instructions\n",
    "\n",
    "sigmas_ = (np.logspace(-1,1,num=10).T)*(np.std(y_train)) #from hw instructions\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#gamma = 0.01*(np.std(y_train)) \n",
    "h,s = HyperParameters(X_train,y_train,hs_,sigmas_)\n",
    "M,V = GPRegression(X_train, y_train, X_test, s, h)\n",
    "print(M,'mean')\n",
    "print(V,'variance')\n",
    "print(np.mean(y_train), 'naive mean')"
   ]
  },
  {
   "source": [
    "#Question 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "        \"age\": [24,53,23,25,32,52,22,43,52,48],\n",
    "        \"salary\": [40000,52000,25000,77000,48000,110000,38000,44000,27000,65000],\n",
    "        \"degree\": [1,0,0,1,1,1,1,0,0,1],\n",
    "    })\n",
    "#lab_enc = preprocessing.LabelEncoder()\n",
    "X = data.iloc[:,:-1] # Features \n",
    "y = data.iloc[:,-1] # Target \n",
    "#X = lab_enc.fit_transform(X_)\n",
    "#y = lab_enc.fit_transform(y_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Building the decision tree\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf = clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|--- salary <= 32500.00\n|   |--- class: 0\n|--- salary >  32500.00\n|   |--- age <= 37.50\n|   |   |--- class: 1\n|   |--- age >  37.50\n|   |   |--- salary <= 58500.00\n|   |   |   |--- class: 0\n|   |   |--- salary >  58500.00\n|   |   |   |--- class: 1\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree.export import export_text\n",
    "show_tree = export_text(clf, feature_names=list(X))\n",
    "print(show_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  feature      info\n1  salary  0.743527\n0     age  0.256473\n"
     ]
    }
   ],
   "source": [
    "info_gain = pd.DataFrame({'feature':X.columns,'info':clf.feature_importances_,})\n",
    "info_gain = info_gain.sort_values('info',ascending=False)\n",
    "print(info_gain)"
   ]
  },
  {
   "source": [
    "The depth of this tree is 3. The information gain at each split is:\n",
    "0.743527, 0.256473\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part b"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|--- salary <= 32500.00\n|   |--- value: [0.00]\n|--- salary >  32500.00\n|   |--- age <= 52.50\n|   |   |--- age <= 37.50\n|   |   |   |--- value: [1.00]\n|   |   |--- age >  37.50\n|   |   |   |--- age <= 45.50\n|   |   |   |   |--- value: [0.00]\n|   |   |   |--- age >  45.50\n|   |   |   |   |--- value: [1.00]\n|   |--- age >  52.50\n|   |   |--- value: [0.00]\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "mult_tree = DecisionTreeRegressor()\n",
    "\n",
    "mult_tree.fit(X,y)\n",
    "mult_tree_rules = export_text(mult_tree, feature_names=list(X))\n",
    "print(mult_tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  feature      info\n1  salary  0.743527\n0     age  0.256473\n"
     ]
    }
   ],
   "source": [
    "mult_info_gain = pd.DataFrame({'feature':X.columns,'info':mult_tree.feature_importances_,})\n",
    "mult_info_gain = mult_info_gain.sort_values('info',ascending=False)\n",
    "print(info_gain)"
   ]
  },
  {
   "source": [
    "#Part c"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A multivariate decision tree would be a poor choice with a small dataset, like the one we have used for this problem, given the increased potential for overfitting. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Problem 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Depth 3, with five attributes. \n",
    "\n",
    "So, for depth three, we have four levels (including the root node), so this gives us 2 raised to the third power, or 6. This is then multiplied by the number of attributes we have, which is 5, so we have 6 times five, giving us 30 possibilities.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 2\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "How many full D depth trees can be drawn? For this question, I used this resource: http://www.mathcs.emory.edu/~cheung/Courses/171/Syllabus/9-BinTree/bin-tree.html\n",
    "\n",
    "The number of nodes at depth D are: $2^d$\n",
    "Then, we multiple this number by the number of attributes, to give us a final formula of: $(A)2^d$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NOTE: For all of the trees in this part, I generated images of them and attached them to the assignment. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "        \"early\": [1,1,0,0,0,0,1,0,0,1,1,0,0,1],\n",
    "        \"Finished hmk\": [1,1,0,1,1,0,0,1,0,0,1,1,0,0],\n",
    "        \"Senior\": [0,1,1,1,1,1,0,0,1,0,1,1,0,0],\n",
    "        \"Likes Coffee\": [0,0,0,0,0,1,0,1,0,0,0,1,0,1],\n",
    "        \"Jedi\":[1,1,0,1,0,1,1,1,1,0,0,1,1,0],\n",
    "        \"A\": [1,1,0,0,1,1,0,1,1,0,1,0,0,1],\n",
    "    })\n",
    "X = data.iloc[:,:-1] # Features \n",
    "y = data.iloc[:,-1] # Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|--- Finished hmk <= 0.50\n|   |--- class: 0\n|--- Finished hmk >  0.50\n|   |--- class: 1\n\n        feature  info\n1  Finished hmk   1.0\n0         early   0.0\n2        Senior   0.0\n3  Likes Coffee   0.0\n4          Jedi   0.0\n"
     ]
    }
   ],
   "source": [
    "#go to depth 1, use entropy. Show decision, # of pos and neg examples, entropy for each node\n",
    "\n",
    "bt = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
    "bt = bt.fit(X,y)\n",
    "\n",
    "from sklearn.tree.export import export_text\n",
    "show_tree = export_text(bt, feature_names=list(X))\n",
    "print(show_tree)\n",
    "\n",
    "info_gain = pd.DataFrame({'feature':X.columns,'info':bt.feature_importances_,})\n",
    "info_gain = info_gain.sort_values('info',ascending=False)\n",
    "print(info_gain)\n",
    "\n"
   ]
  },
  {
   "source": [
    "For visualizing the binary tree, I adapted code from this resource: http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import render\n",
    "\n",
    "features = ['early', 'Finished hwk', 'Senior', 'Likes Coffee', 'Jedi']\n",
    "\n",
    "def visualize_tree(tree, feature_names):\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = ['dot', '-Tpng', 'dt.dot', '-o', 'dt.png']\n",
    "    \n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Something's wrong\")\n",
    "\n",
    "visualize_tree(bt,features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|--- Finished hmk <= 0.50\n|   |--- Likes Coffee <= 0.50\n|   |   |--- class: 0\n|   |--- Likes Coffee >  0.50\n|   |   |--- class: 1\n|--- Finished hmk >  0.50\n|   |--- early <= 0.50\n|   |   |--- class: 0\n|   |--- early >  0.50\n|   |   |--- class: 1\n\n        feature      info\n3  Likes Coffee  0.531564\n0         early  0.330206\n1  Finished hmk  0.138230\n2        Senior  0.000000\n4          Jedi  0.000000\n"
     ]
    }
   ],
   "source": [
    "#Same thing, depth 2\n",
    "\n",
    "bt2 = DecisionTreeClassifier(criterion=\"entropy\",max_depth=2)\n",
    "bt2 = bt2.fit(X,y)\n",
    "\n",
    "from sklearn.tree.export import export_text\n",
    "show_tree = export_text(bt2, feature_names=list(X))\n",
    "print(show_tree)\n",
    "\n",
    "info_gain = pd.DataFrame({'feature':X.columns,'info':bt2.feature_importances_,})\n",
    "info_gain = info_gain.sort_values('info',ascending=False)\n",
    "print(info_gain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(bt2,features)\n"
   ]
  },
  {
   "source": [
    "Attached are the images of all the trees. Here is the information for each tree.\n",
    "\n",
    "Depth 1:\n",
    "\n",
    "positive examples: 7\n",
    "\n",
    "negative examples: 7 \n",
    "\n",
    "entropy for each node: 0.985, 0.863\n",
    "\n",
    "Depth 2:\n",
    "\n",
    "Positive examples: 7 (sub-examples: 2,5)\n",
    "\n",
    "Negative examples: 7 (sub-examples: 4,3)\n",
    "\n",
    "Entropy for each node: 0.985, next level: 0.985, 0.863, next level: 0.722, 0, 1.0, 0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|--- Finished hmk <= 0.50\n|   |--- Likes Coffee <= 0.50\n|   |   |--- Senior <= 0.50\n|   |   |   |--- class: 0\n|   |   |--- Senior >  0.50\n|   |   |   |--- class: 0\n|   |--- Likes Coffee >  0.50\n|   |   |--- class: 1\n|--- Finished hmk >  0.50\n|   |--- early <= 0.50\n|   |   |--- Jedi <= 0.50\n|   |   |   |--- class: 1\n|   |   |--- Jedi >  0.50\n|   |   |   |--- class: 0\n|   |--- early >  0.50\n|   |   |--- class: 1\n\n        feature      info\n3  Likes Coffee  0.363670\n0         early  0.225910\n2        Senior  0.178091\n4          Jedi  0.137759\n1  Finished hmk  0.094570\n"
     ]
    }
   ],
   "source": [
    "#Depth 3\n",
    "\n",
    "bt3 = DecisionTreeClassifier(criterion=\"entropy\",max_depth=3)\n",
    "bt3 = bt3.fit(X,y)\n",
    "\n",
    "from sklearn.tree.export import export_text\n",
    "show_tree = export_text(bt3, feature_names=list(X))\n",
    "print(show_tree)\n",
    "\n",
    "info_gain = pd.DataFrame({'feature':X.columns,'info':bt3.feature_importances_,})\n",
    "info_gain = info_gain.sort_values('info',ascending=False)\n",
    "print(info_gain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'bt3.gv.png'"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "visualize_tree(bt3,features)\n",
    "\n",
    "from graphviz import Source\n",
    "temp = \"\"\"\n",
    "digraph Tree {\n",
    "node [shape=box] ;\n",
    "0 [label=\"Finished hwk <= 0.5\\nentropy = 0.985\\nsamples = 14\\nvalue = [6, 8]\"] ;\n",
    "1 [label=\"Likes Coffee <= 0.5\\nentropy = 0.985\\nsamples = 7\\nvalue = [4, 3]\"] ;\n",
    "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
    "2 [label=\"Senior <= 0.5\\nentropy = 0.722\\nsamples = 5\\nvalue = [4, 1]\"] ;\n",
    "1 -> 2 ;\n",
    "3 [label=\"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"] ;\n",
    "2 -> 3 ;\n",
    "4 [label=\"entropy = 1.0\\nsamples = 2\\nvalue = [1, 1]\"] ;\n",
    "2 -> 4 ;\n",
    "5 [label=\"entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]\"] ;\n",
    "1 -> 5 ;\n",
    "6 [label=\"early <= 0.5\\nentropy = 0.863\\nsamples = 7\\nvalue = [2, 5]\"] ;\n",
    "0 -> 6 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
    "7 [label=\"Jedi <= 0.5\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]\"] ;\n",
    "6 -> 7 ;\n",
    "8 [label=\"entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]\"] ;\n",
    "7 -> 8 ;\n",
    "9 [label=\"entropy = 0.918\\nsamples = 3\\nvalue = [2, 1]\"] ;\n",
    "7 -> 9 ;\n",
    "10 [label=\"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\"] ;\n",
    "6 -> 10 ;\n",
    "}\n",
    "\"\"\"\n",
    "s = Source(temp, filename=\"bt3.gv\", format=\"png\")\n",
    "s.view()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Positive Examples: 7, next split: (5,2), next split: (3,2)\n",
    "\n",
    "Negative Examples: 7, next split (4,3), next split: (1,3)\n",
    "\n",
    "Entropy for each node: 0.985, next split: 0.985, 0.863, next split: 0.722, 0, 1.0, 0, next split: 0.0, 1, 0.0, 0.918"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Because the data set is so small, in this case, avoiding overfitting and high variance is the most important thing. Having fewer numbers of splits in this case would help these issues."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}