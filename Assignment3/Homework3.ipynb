{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)",
   "display_name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "7c081f5da1bfee50c0186fa1fb9a35fab0508e5e9efe67840514a29b425454b6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset of points\n",
    "#f=open('/Users/benso/Desktop/Projects/MachineLearning/Assignment3/hw3_data2.txt',\"r\")\n",
    "f=open('/Users/lvbenson/Research_Projects/MachineLearning/Assignment3/hw3_data2.txt',\"r\")\n",
    "\n",
    "lines = f.readlines()\n",
    "result=[]\n",
    "for x in lines:\n",
    "    result.append(x.split(',')[0])\n",
    "f.close()\n",
    "\n",
    "target = []\n",
    "for sign in result:\n",
    "    if sign == '+':\n",
    "        target.append(1)\n",
    "    else:\n",
    "        target.append(-1)\n",
    "\n",
    "#f=open('/Users/benso/Desktop/Projects/MachineLearning/Assignment3/hw3_data2.txt',\"r\")\n",
    "f=open('/Users/lvbenson/Research_Projects/MachineLearning/Assignment3/hw3_data2.txt',\"r\")\n",
    "lines1=f.readlines()\n",
    "result2=[]\n",
    "for x in lines1:\n",
    "    result2.append(x.split(',')[1])\n",
    "f.close()\n",
    "\n",
    "x_coords = []\n",
    "for coord in result2:\n",
    "    x_coords.append(coord)\n",
    "\n",
    "#f=open('/Users/benso/Desktop/Projects/MachineLearning/Assignment3/hw3_data2.txt',\"r\")\n",
    "f=open('/Users/lvbenson/Research_Projects/MachineLearning/Assignment3/hw3_data2.txt',\"r\")\n",
    "lines2=f.readlines()\n",
    "result3=[]\n",
    "for x in lines2:\n",
    "    x_ = x.split(',')[2].rstrip('\\n')\n",
    "    result3.append(x_)\n",
    "y_coords = []\n",
    "for coord in result3:\n",
    "    y_coords.append(coord)\n",
    "data = []\n",
    "for x,y in zip(x_coords,y_coords):\n",
    "    data.append([x,y])\n",
    "f.close()\n",
    "\n",
    "X = np.array(data,dtype=float)\n",
    "Y = np.array(target,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test, train vectors\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X,gamma):\n",
    "    K = np.zeros((X.shape[0],X.shape[0]))\n",
    "    for i in range(X.shape[0]): \n",
    "        for j in range(X.shape[0]):\n",
    "            K[i,j] = np.exp(-gamma*np.linalg.norm(X[i]-X[j])**2) # rbf kernel alg, kinda like euclid distance. All this function does it calculate the kernel, to be used later.\n",
    "    return K"
   ]
  },
  {
   "source": [
    "In reference to this document: http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf, and this document: https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d, the following is the implementation of a function to determine the cost for each class. This function uses the weight vector calculated in the SVM function, and then goes through the dataset to determine the cost based on the corresponding element classified in the target vector (Y). \n",
    "\n",
    "As I learned from the towardsdatascience document, the following is the SVM cost function:\n",
    "\n",
    "$Cost(h_{\\theta},(x),y)=max(0,1-\\theta^{T}x)$ if y = 0 \n",
    "and\n",
    "$Cost(h_{\\theta},(x),y)=max(0,1+\\theta^{T}x)$ if y = 1\n",
    "\n",
    "Where y is our classification. In my case, the two possible classifications are -1 and 1.\n",
    "\n",
    "So, the final loss function for the non-linear SVM is as follows (and is calculated in the loss calculation function). Note that for this implementation, I chose to use a regularized C term of just 1, but this can be manipulated. This could be used to see how, over time, the loss improves as the line wiggles less and less. If I have time, I'll try to visualize this loss function.\n",
    "\n",
    "(also retrieved from the towardsdatascience document):\n",
    "\n",
    "$J(\\theta)=C[\\sum_{i=1}^{m}y^{i}Cost_{1}(\\theta^{T}(f^{(i)})+(1-y^{(i)})Cost_{0}(\\theta^{T}(f^{(i)})]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_Calculation(w,K,X,Y,reg_term=1,C=1): #K is 2700 x 2700 matrix (for training data), x is 2700x2, w is 2700x1 (training data)\n",
    "    #Hinge loss determinism\n",
    "    L_list = []\n",
    "    size_y = range(len(y))\n",
    "    for i in size_y:\n",
    "        if -Y[i]*classification(i,w,Y,K)<1:\n",
    "            L_list.append(i)\n",
    "\n",
    "    for i in size_y:\n",
    "        for j in L_list:\n",
    "            sub_grad_help = sum([Y[j]*K[j][i]])\n",
    "        sub_grad_list = [(-Y[i]*sub_grad_help)]\n",
    "        w_change = np.array(sub_grad_list)\n",
    "\n",
    "    return w_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our overall classification function that we'll use to check to see if the point is classified as positive or negative.\n",
    "def classification(index_x,w,Y,K):\n",
    "    size_y = range(len(Y))\n",
    "    class_func = 0\n",
    "    for i in size_y:\n",
    "        class_func += Y[i]*K[index_x][i]*w[i]\n",
    "    return class_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X,Y,K,epochs=25,learn_rate=1,reg_term=1,C=1,Gamma=1):\n",
    "    #initialize random weights\n",
    "    w = np.zeros(X.shape[0])\n",
    "    for epoch in range(epochs):\n",
    "        w_change = Loss_Calculation(w,K,X,Y,reg_term,C)\n",
    "        w = np.subtract(w,w_change) #account for the wiggle in the w vector\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X,Y,epochs=25,learn_rate=1,reg_term=1,C=1,Gamma=1):\n",
    "    K = rbf_kernel(X,Gamma)\n",
    "    w_vector = SVM(X,Y,K,epochs,learn_rate)\n",
    "    #need to classify everything in X \n",
    "    #things to classify:\n",
    "    classifications = [classification(i,w_vector,Y,K) for i in range(len(X))]\n",
    "    \"\"\"\n",
    "    size = range(len(X))\n",
    "\n",
    "    classify_list = []\n",
    "    for index_x in size:\n",
    "        classify_list.append(classification(index_x,w_vector,Y,K,))\n",
    "    \"\"\"\n",
    "    return classifications,w_vector\n",
    "\n",
    "classes,w = pipeline(x_train,y_train)\n",
    "correct_classify = []\n",
    "incorrect_classify = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9533333333333334  :accuracy of training data\n"
     ]
    }
   ],
   "source": [
    "#check out the training data accuracy\n",
    "\n",
    "for i,y in zip(classes,y_train):\n",
    "    if i > 0 and y == 1:\n",
    "        correct_classify.append(1)\n",
    "    elif i < 0 and y == -1:\n",
    "        correct_classify.append(1)\n",
    "    else:\n",
    "        incorrect_classify.append(1)\n",
    "total_correct = (len(correct_classify)) / (len(correct_classify) + len(incorrect_classify))\n",
    "print(total_correct,' :accuracy of training data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9711111111111111  :accuracy of test data\n"
     ]
    }
   ],
   "source": [
    "#check out the test data accuracy. only calculate kernel and classes, use the w that is already calculated\n",
    "K = rbf_kernel(x_test,1)\n",
    "classifications_test = [classification(i,w,y_test,K) for i in range(len(x_test))]\n",
    "\n",
    "correct_classify_test = []\n",
    "incorrect_classify_test = []\n",
    "\n",
    "#check out the training data accuracy\n",
    "\n",
    "for i,y in zip(classifications_test,y_test):\n",
    "    if i > 0 and y == 1:\n",
    "        correct_classify_test.append(1)\n",
    "    elif i < 0 and y == -1:\n",
    "        correct_classify_test.append(1)\n",
    "    else:\n",
    "        incorrect_classify_test.append(1)\n",
    "total_correct_test = (len(correct_classify_test)) / (len(correct_classify_test) + len(incorrect_classify_test))\n",
    "print(total_correct_test,' :accuracy of test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}